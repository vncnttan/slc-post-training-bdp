{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==2.4.4\n",
      "  Using cached pyspark-2.4.4.tar.gz (215.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"d:\\BINUS\\BLUEJACK_nar\\04. Post Training\\29. Big Data Processing\\Materi\\PTSpark\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"d:\\BINUS\\BLUEJACK_nar\\04. Post Training\\29. Big Data Processing\\Materi\\PTSpark\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"d:\\BINUS\\BLUEJACK_nar\\04. Post Training\\29. Big Data Processing\\Materi\\PTSpark\\.venv\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "        File \"C:\\Users\\vince\\AppData\\Local\\Temp\\pip-build-env-6xx8ym67\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 380, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\vince\\AppData\\Local\\Temp\\pip-build-env-6xx8ym67\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 488, in run_setup\n",
      "          self).run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\vince\\AppData\\Local\\Temp\\pip-build-env-6xx8ym67\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 338, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 156, in <module>\n",
      "      AttributeError: module 'pypandoc' has no attribute 'convert'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==2.4.4\n",
    "# %pip install pyspark[sql]==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "context = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"We are learning Big Data Processing\",\n",
    " \"Now We are in Spark\",\n",
    " \"We are learning RDD\"]\n",
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddSentences = context.parallelize(sentences)\n",
    "\n",
    "print(type(rddSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lihat semua data\n",
    "print(rddSentences.collect())\n",
    "\n",
    "# Lihat data pertama\n",
    "print(rddSentences.first())\n",
    "\n",
    "# Lihat beberapa data teratas\n",
    "print(rddSentences.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(val):\n",
    "    return val\n",
    "\n",
    "# Map\n",
    "mappedSentences = rddSentences.map(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "# Flat Map\n",
    "mappedSentences = mappedSentences.flatMap(lambda sentence: sentence)\n",
    "\n",
    "mappedSentences = mappedSentences.map(lambda word: (word, 1))\n",
    "\n",
    "print(mappedSentences.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce\n",
    "result = mappedSentences.reduceByKey(lambda curr, acum: curr + acum)\n",
    "result.filter(lambda word: word[1] > 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlibNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 1.0 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     -------------------------------------- 965.4/965.4 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\binus\\bluejack_nar\\04. post training\\29. big data processing\\materi\\ptspark\\.venv\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\binus\\bluejack_nar\\04. post training\\29. big data processing\\materi\\ptspark\\.venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\binus\\bluejack_nar\\04. post training\\29. big data processing\\materi\\ptspark\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting typing-extensions (from kiwisolver>=1.0.1->matplotlib)\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\binus\\bluejack_nar\\04. post training\\29. big data processing\\materi\\ptspark\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 55.8/55.8 kB 576.7 kB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, pyparsing, pillow, fonttools, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.5 matplotlib-3.5.3 pillow-9.5.0 pyparsing-3.1.1 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
